# johnadamsbab

The neural network model was subjected to a series of systematic alterations in the conducted trials, each of which revealed unique effects on its performance. The first model, which had two hidden layers and followed notebook construction guidelines, demonstrated respectable accuracy rates on test, validation, and training datasets. Reducing the batch size to 64 revealed an unexpected improvement in accuracy, defying the popular belief that bigger batch sizes are preferable. Increasing the number of epochs to 200 subsequently led to much higher accuracy levels, confirming the intuitive belief that longer exposure to the training data improves the model's processing speed. But this rising tendency peaked, highlighting the need for an ideal epoch threshold. Interestingly, albeit less dramatically than with other interventions, the inclusion of a third hidden layer produced gains.








